# DA0028 2021_Aug_17 Analysis.1
# worked for about 2 hours on 18 and 19 Aug 2021


## change directories
    
    cd /nfs/scistore16/itgrp/bioinf/projects/DA0028/17_Aug_2021/analysis.1/

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0028/17_Aug_2021/analysis.1/raw

## rsync over the data from /fs3/ biogrp to scistore16

    rsync --stats --progress --archive --append-verify /fs3/group/biogrp/deBono/DA0028/17_Aug_2021/analysis.1/raw/ . 


## make symbolic links for easier data processing

    ln -fs 170310_AX8180_single_worm/170310_*R1* 170310.mate1.fastq.gz
    ln -fs 170310_AX8180_single_worm/170310_*R2* 170310.mate2.fastq.gz

    ln -fs 170309_N2_single_worm/170309_*R1* 170309.mate1.fastq.gz
    ln -fs 170309_N2_single_worm/170309_*R2* 170309.mate2.fastq.gz

    ln -fs 170283_AX8180_bulk/170283_*R1* 170283.mate1.fastq.gz
    ln -fs 170283_AX8180_bulk/170283_*R2* 170283.mate2.fastq.gz
 
    ln -fs 170282_N2_bulk/170282_*R1* 170282.mate1.fastq.gz
    ln -fs 170282_N2_bulk/170282_*R2* 170282.mate2.fastq.gz


## change directories again

cd /nfs/scistore16/itgrp/bioinf/projects/DA0028/17_Aug_2021/analysis.1/

## this is the config file for running the snakemake pipeline for two samples

config2.yaml

    # files with {dataset}.{mate1,mate2}.fastq.gz or {dataset}.bam must be in the directory "raw"
    datasets: [170282,
               170283]
    threads: "8"
    # bbduk parameters
    bbduk-params: "ktrim=r k=23 mink=11 hdist=1 tpe tbo qtrim=rl trimq=15"
    # path to a fasta file with the adapter sequences to be used for clipping
    bbduk-fasta: "/nfs/scistore16/itgrp/jelbers/bin/bbmap-38.90/resources/adapters.fa"
    # path to a fasta file with the adapter sequences to be used for clipping
    reference-fasta: "/nfs/scistore14/rcsb/pub/biogrp/ReferenceData/c.elegans/WBcel235-ce11/ensembl/Caenorhabditis_elegans.WBcel235.dna.toplevel.fa.gz"

## this is the config file for running the snakemake pipeline for the other two samples

config.yaml

    # files with {dataset}.{mate1,mate2}.fastq.gz or {dataset}.bam must be in the directory "raw"
    datasets: [170309,
               170310]
    threads: "8"
    # bbduk parameters
    bbduk-params: "ktrim=r k=23 mink=11 hdist=1 tpe tbo qtrim=rl trimq=15"
    # path to a fasta file with the adapter sequences to be used for clipping
    bbduk-fasta: "/nfs/scistore16/itgrp/jelbers/bin/bbmap-38.90/resources/adapters.fa"
    # path to a fasta file with the adapter sequences to be used for clipping
    reference-fasta: "/nfs/scistore14/rcsb/pub/biogrp/ReferenceData/c.elegans/WBcel235-ce11/ensembl/Caenorhabditis_elegans.WBcel235.dna.toplevel.fa.gz"


## this is the cluster config file for SLURM

cluster.json

    {
        "__default__":
        {
            "account": "jelbers",
            "time": "2:00:0",
            "mem": "12G",
            "job-name": "{rule}",
        },

        "bwa-mem2":
        {
            "account": "jelbers",
            "time": "2:00:0",
            "mem": "24G",
            "job-name" : "{rule}",
        },

        "octopus":
        {
            "account": "jelbers",
            "time": "04:00:0",
            "mem": "16G",
            "job-name" : "{rule}",
        }
    }


## this is the Snakefile for snakemake assuming 50-bp paired-end reads for
## Illumina NovaSeq with PCR library prep
### the --OPTICAL_DUPLICATE_PIXEL_DISTANCE needs to be changed if other Illumina
### sequencer is used
### also see https://luntergroup.github.io/octopus/docs/guides/errorModels
### to use a non-default octopus error model

Snakefile-bwa-octopus-split

    # specify name of external config-file
    configfile: "config.yaml"

    # import custom global parameter from config-file
    DATASETS = config["datasets"]
    THREADS = config["threads"]
    scattergather:
        split=28

    # list of rules which are not deployed to slurm
    localrules: all, fastqc, clean, adapters_fasta, rm_duplicates, mapping, make_test_bed, make_bed, snps


    # final target rule to produce all sub targets
    rule all:
        input:
            fastqc = "logs/fastqc.done",
            mapping = "logs/bwa-mem2.done",
            rm_duplicates = "logs/picard.done",
            snps = "logs/octopus.done"

    # step 1: check read quality of raw reads
    rule fastqc_check:
        input:
            mate1 = "raw/{sample}.mate1.fastq.gz",
            mate2 = "raw/{sample}.mate2.fastq.gz"
        output:
            mate1 = "qc/fastqc_raw/{sample}.mate1_fastqc.html",
            mate2 = "qc/fastqc_raw/{sample}.mate2_fastqc.html"
        params:
            threads = THREADS,
            dir = "qc/fastqc_raw"
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load fastqc/0.11.7
            fastqc --outdir {params.dir} --thread {params.threads} {input.mate1} {input.mate2}  > /dev/null 2> /dev/null
            '''


    # step 2: quality and adapter trimming of reads with bbduk
    rule adapters_fasta:
        input: config["bbduk-fasta"]
        output: "auxData/bbduk-fasta.fa"
        shell:
            '''
            cp {input} {output}
            '''

    rule bbduk:
        input:
            mate1 = "raw/{sample}.mate1.fastq.gz",
            mate2 = "raw/{sample}.mate2.fastq.gz",
            adapter = "auxData/bbduk-fasta.fa"
        output:
            mate1 = "fastq/{sample}-trimmed.mate1.fastq.gz",
            mate2 = "fastq/{sample}-trimmed.mate2.fastq.gz"
        log:
            err = "logs/bbduk/{sample}.log"
        params:
            threads = THREADS,
            params = config["bbduk-params"]       
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load bbtools/38.82
            module load bcftools/1.13
            bbduk.sh threads={params.threads} in1={input.mate1} in2={input.mate2} out1={output.mate1} out2={output.mate2} ref={input.adapter} {params.params} > {log.err} 2>&1
            '''

    # step 3: check read quality of trimmed reads
    rule fastqc_recheck:
        input:
            mate1 = "fastq/{sample}-trimmed.mate1.fastq.gz",
            mate2 = "fastq/{sample}-trimmed.mate2.fastq.gz"
        output:
            mate1 = "qc/fastqc_trimmed/{sample}-trimmed.mate1_fastqc.html",
            mate2 = "qc/fastqc_trimmed/{sample}-trimmed.mate2_fastqc.html"
        params:
            threads = THREADS
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load fastqc/0.11.7
            fastqc --outdir qc/fastqc_trimmed/ --thread {params.threads} {input.mate1} {input.mate2}  > /dev/null 2> /dev/null
            '''

    rule fastqc:
        input:
            fastq1 = expand("qc/fastqc_trimmed/{dataset}-trimmed.mate1_fastqc.html", dataset=DATASETS),
            fastq2 = expand("qc/fastqc_trimmed/{dataset}-trimmed.mate2_fastqc.html", dataset=DATASETS),
            fastq3 = expand("qc/fastqc_raw/{dataset}.mate1_fastqc.html", dataset=DATASETS),
            fastq4 = expand("qc/fastqc_raw/{dataset}.mate2_fastqc.html", dataset=DATASETS)
        output: "logs/fastqc.done"
        shell:
            '''
            touch {output}
            '''

    # step 4: map reads to reference genome
    rule reference_fasta:
        input: config["reference-fasta"]
        output: 
            reference = "auxData/reference-fasta.fa",
            index = "auxData/reference-fasta.fa.fai"
        log: "logs/reference/bwa-mem2-index.log"
        shell:
            '''
            # get C elegans reference from Fabian (from Ensembl presumably)
            # see config.yaml on path to reference
            module load seqtk/20210818
            module load samtools/1.13
            module load bwa-mem2/2.2.1
            zcat {input} |seqtk seq -UC > {output.reference}
            bwa-mem2 index -p {output.reference} {output.reference} > {log} 2>&1
            samtools faidx {output.reference}
            '''

    rule bwa_mem2:
        input:
            mate1 = "fastq/{sample}-trimmed.mate1.fastq.gz",
            mate2 = "fastq/{sample}-trimmed.mate2.fastq.gz",
            reference = "auxData/reference-fasta.fa"
        output:
            bam = "bam/{sample}.bam",
            index = "bam/{sample}.bam.bai"
        log:
            err = "logs/bwa-mem2/{sample}.log"
        params:
            threads = THREADS
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load samtools/1.13
            module load bwa-mem2/2.2.1
            bwa-mem2 mem -t {params.threads} \
            -R "@RG\tID:{wildcards.sample}\tSM:{wildcards.sample}\tPL:ILLUMINA\tPU:barcode" \
            {input.reference} {input.mate1} {input.mate2} 2> {log.err}| samtools sort -@ {params.threads} -o {output.bam} -  2>/dev/null
            samtools index -@ {params.threads} {output.bam}
            '''

    rule mapping:
        input:
            interleaved = expand("bam/{dataset}.bam", dataset=DATASETS)
        output: "logs/bwa-mem2.done"
        shell:
            '''
            touch {output}
            '''

    # step 5: mark pcr duplicates and optical duplicates for NovaSeq (--OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500)
    rule picard:
        input:
            bam = "bam/{sample}.bam",
            index = "bam/{sample}.bam.bai"
        output:
            bam1 = temp("bam/{sample}2.bam"),
            bam2 = "bam/{sample}-md.bam",
            index = "bam/{sample}-md.bai",
            metrics = "bam/{sample}-metrics.txt"
        log:
            err = "logs/picard/{sample}.log"
        shell:
            '''
            module load samtools/1.13
            samtools reheader \
            <(samtools view -H {input.bam} |\
            perl -pe "s/-R \@RG\tID:{wildcards.sample}\tSM:{wildcards.sample}\tPL:ILLUMINA\tPU:barcode/-R \@RG ID:{wildcards.sample} SM:{wildcards.sample} PL:ILLUMINA PU:barcode/g"|\
            perl -pe "s/PP:illumina/PP:bwa-mem2/g") \
            {input.bam} > {output.bam1}

            module unload samtools/1.13
            module load picard/2.25.6
            java -jar $PICARD MarkDuplicatesWithMateCigar \
            --METRICS_FILE {output.metrics} \
            --INPUT {output.bam1} \
            --OUTPUT {output.bam2} \
            --ASSUME_SORT_ORDER coordinate \
            --OPTICAL_DUPLICATE_PIXEL_DISTANCE 2500 \
            --CREATE_INDEX true > {log.err} 2>&1
            '''

    rule rm_duplicates:
        input:
            bam = expand("bam/{dataset}-md.bam", dataset=DATASETS)
        output: "logs/picard.done"
        shell:
            '''
            touch {output}
            '''

    # step 6 call SNPs
    rule make_test_bed:
        input:
            "auxData/reference-fasta.fa.fai"
        output:
            "vcf/test.bed"
        log:
            "logs/octopus/test.bed.err"
        shell:
            '''
            module load bedtools/2.29.2
            bedtools makewindows -g {input} -n 4 > {output} 2> {log}
            '''

    rule make_bed:
        input:
            "vcf/test.bed"
        output:
            scatter.split("vcf/{{sample}}_{scatteritem}.bed")
        run:
            shell("touch {output}")
            for i in range(1, 29):
                shell("sed -n {i}p {input} > vcf/{wildcards.sample}_{i}-of-28.bed")

    rule octopus:
        input:
            bed = "vcf/{sample}_{scatteritem}.bed",
            bam = "bam/{sample}-md.bam",
            reference = "auxData/reference-fasta.fa"
        output:
            vcf1 = temp("vcf/{sample}_{scatteritem}.vcf"),
            vcf2 = "vcf/{sample}_{scatteritem}.vcf.gz",
            index = "vcf/{sample}_{scatteritem}.vcf.gz.tbi"
        log:
            err = "logs/octopus/{sample}_{scatteritem}.log"
        params:
            threads = THREADS
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load octopus/0.7.4
            octopus --regions-file {input.bed} --temp-directory-prefix vcf/{wildcards.sample}_{wildcards.scatteritem}.temp --reference {input.reference} --reads {input.bam} --threads {params.threads} -o {output.vcf1} > {log.err} 2>&1
            module load bcftools/1.13
            bgzip -@{params.threads} {output.vcf1} -c > {output.vcf2}
            tabix -p vcf {output.vcf2}
            '''

    rule gather_vcf:
        input:
            gather.split("vcf/{{sample}}_{scatteritem}.vcf.gz")
        output:
            "vcf/{sample}.vcf"
        params:
            threads = THREADS
        shell:
            '''
            module load bcftools/1.13
            bcftools concat --threads {params.threads} {input} > {output}
            '''

    rule snps:
        input:
            vcf = expand("vcf/{sample}.vcf", sample=DATASETS)
        output: "logs/octopus.done"
        shell:
            '''
            touch {output}
            '''

    # rule to remove all workflow intermediate and results files, but the logs
    rule clean:
        shell:
            '''
            rm -rf vcf/        
            rm -rf bam/        
            rm -rf fastq/
            rm -rf qc/
            rm -rf auxData/
            rm -rf logs/*.done
            '''


## actually run snakemake

### activate snakemake-6.3.0 conda environment
 
    conda activate snakemake-6.3.0

### make the DAG graph to view what will happen

    snakemake --snakefile Snakefile-bwa-octopus-split --dryrun  --dag all | dot -Tsvg > dag.svg

### actually run the pipeline

    snakemake --snakefile Snakefile-bwa-octopus-split --printshellcmds --latency-wait 60 --local-cores 1 --cores all --cluster-config cluster.json --cluster "sbatch --export=NONE --no-requeue --job-name {cluster.job-name} --mem={cluster.mem} --time={cluster.time} --cores={THREADS} --constraint=avx2" all > snakemake-picard-bwa-octopus-split.log 2>&1 &


### this take the same as before but for the "other" two bulk and not single worm samples
### not needed if you put all the samples in the first config file

    perl -pe "s/config.yaml/config2.yaml/g" Snakefile-bwa-octopus-split > Snakefile-bwa-octopus-split-bulk

    rm -f logs/*.done

    conda activate snakemake-6.3.0

    snakemake --snakefile Snakefile-bwa-octopus-split-bulk --dryrun  --dag all | dot -Tsvg > dag-bulk.svg
    snakemake --snakefile Snakefile-bwa-octopus-split-bulk --printshellcmds --latency-wait 60 --local-cores 1 --cores all --cluster-config cluster.json --cluster "sbatch --export=NONE --no-requeue --job-name {cluster.job-name} --mem={cluster.mem} --time={cluster.time} --cores={THREADS} --constraint=avx2" all > snakemake-picard-bwa-octopus-split-bulk-finish.log 2>&1 &



    conda deactivate


## collect whole-genome sequencing metrics to plot with multiqc

### READ_LENGTH= would need to be set to 150 for the new project,
### this could also be added to the Snakefile instead of manually run here

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0028/17_Aug_2021/analysis.1/bam

    module load picard/2.13.2
    java -Xmx60g -jar $PICARD CollectWgsMetrics \
    I=170310-md.bam \
    O=170310-md.bam_wgs_metrics.txt \
    R=../auxData/reference-fasta.fa \
    INCLUDE_BQ_HISTOGRAM=false \
    READ_LENGTH=50

    java -Xmx60g -jar $PICARD CollectWgsMetrics \
    I=170309-md.bam \
    O=170309-md.bam_wgs_metrics.txt \
    R=../auxData/reference-fasta.fa \
    INCLUDE_BQ_HISTOGRAM=false \
    READ_LENGTH=50


    java -Xmx60g -jar $PICARD CollectWgsMetrics \
    I=170283-md.bam \
    O=170283-md.bam_wgs_metrics.txt \
    R=../auxData/reference-fasta.fa \
    INCLUDE_BQ_HISTOGRAM=false \
    READ_LENGTH=50

    java -Xmx60g -jar $PICARD CollectWgsMetrics \
    I=170282-md.bam \
    O=170282-md.bam_wgs_metrics.txt \
    R=../auxData/reference-fasta.fa \
    INCLUDE_BQ_HISTOGRAM=false \
    READ_LENGTH=50


    conda deactivate

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0028/17_Aug_2021/analysis.1
    conda activate python-3.9.4

    /nfs/scistore16/itgrp/jelbers/.local/bin/multiqc --interactive --title updated-with-bulk bam/ qc/




## other desired steps that would not be needed for I think for Thanh's new analysis

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0028/17_Aug_2021/analysis.1/vcf

    module load bcftools/1.13
    grep "PASS\|^#" 170310.vcf > 170310.pass.vcf
    grep "PASS\|^#" 170309.vcf > 170309.pass.vcf
    bgzip 170309.pass.vcf
    bgzip 170310.pass.vcf 
    tabix -p vcf 170310.pass.vcf.gz 
    tabix -p vcf 170309.pass.vcf.gz 
    bcftools merge 170309.pass.vcf.gz 170310.pass.vcf.gz |bgzip > merged.vcf.gz
    tabix -p vcf merged.vcf.gz
    bcftools view -m2 -M2 --genotype ^miss merged.vcf.gz |perl -pe "s/VCFv4.3/VCFv4.2/g" |perl -pe "s/^(\S+)\t(\S+)\t\./\1\t\2\t\1_\2/g" > merged.biallelic.no.missing.genotypes.vcf

    module load plink/1.90
    plink --freqx --vcf merged.biallelic.no.missing.genotypes.vcf --allow-extra-chr
    grep -P "\t2" plink.frqx > different.from.reference

    grep -vf <(tail -n+2 different.from.reference|cut -f 2) merged.biallelic.no.missing.genotypes.vcf > merged.biallelic.no.missing.genotypes.realsnps.vcf




    module load bcftools/1.13
    grep "PASS\|^#" 170310.vcf > 170310.pass.vcf
    grep "PASS\|^#" 170309.vcf > 170309.pass.vcf
    bgzip -f 170309.pass.vcf
    bgzip -f 170310.pass.vcf 
    tabix -fp vcf 170310.pass.vcf.gz 
    tabix -fp vcf 170309.pass.vcf.gz 
    bcftools merge 170309.pass.vcf.gz 170310.pass.vcf.gz |bgzip > merged.vcf.gz
    tabix -p vcf merged.vcf.gz
    bcftools view -m2 -M2 --genotype ^miss merged.vcf.gz |perl -pe "s/VCFv4.3/VCFv4.2/g" |perl -pe "s/^(\S+)\t(\S+)\t\./\1\t\2\t\1_\2/g" > merged.biallelic.no.missing.genotypes.vcf

    module load plink/1.90
    plink --freqx --vcf merged.biallelic.no.missing.genotypes.vcf --allow-extra-chr
    grep -P "\t2" plink.frqx > different.from.reference

    grep -vf <(tail -n+2 different.from.reference|cut -f 2) merged.biallelic.no.missing.genotypes.vcf > 170309-vs-170310.realsnps.vcf



    # 

    module load bcftools/1.13
    grep "PASS\|^#" 170282.vcf > 170282.pass.vcf
    grep "PASS\|^#" 170283.vcf > 170283.pass.vcf
    bgzip -f 170283.pass.vcf
    bgzip -f 170282.pass.vcf 
    tabix -fp vcf 170282.pass.vcf.gz 
    tabix -fp vcf 170283.pass.vcf.gz 
    bcftools merge 170283.pass.vcf.gz 170282.pass.vcf.gz |bgzip > merged2.vcf.gz
    tabix -fp vcf merged2.vcf.gz
    bcftools view -m2 -M2 --genotype ^miss merged2.vcf.gz |perl -pe "s/VCFv4.3/VCFv4.2/g" |perl -pe "s/^(\S+)\t(\S+)\t\./\1\t\2\t\1_\2/g" > merged2.biallelic.no.missing.genotypes.vcf

    module load plink/1.90
    plink --freqx --vcf merged2.biallelic.no.missing.genotypes.vcf --allow-extra-chr --out merged2
    grep -P "\t2" merged2.frqx > different2.from.reference

    grep -vf <(tail -n+2 different2.from.reference|cut -f 2) merged2.biallelic.no.missing.genotypes.vcf > 170282-vs-170283.realsnps.vcf
    bedtools intersect -a 170309-vs-170310.realsnps.vcf -b 170282-vs-170283.realsnps.vcf -header > 170309-vs-170310.realsnps--intersection--170282-vs-170283.realsnps.vcf

    zip DA0028-2021_Aug_17_analysis1.zip 170309-vs-170310.realsnps--intersection--170282-vs-170283.realsnps.vcf 170309-vs-170310.realsnps.vcf 170282-vs-170283.realsnps.vcf
