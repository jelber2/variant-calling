# DA0033 Variant Mapping Analysis of C. elegans EMS-induced mutants

---

## Analysis.1 Call SNPs with octopus

### config file

config2.yaml

    # files with {dataset}.{mate1,mate2}.fastq.gz or {dataset}.bam must be in the directory "raw"
    datasets: [177106,
               177107,
               177108,
               177109,
               177110,
               177111,
               177112,
               177113,
               177114,
               177115,
               177116,
               177117,
               177118,
               177119,
               177120,
               177121,
               177122,
               177123,
               177124,
               177125,
               177126,
               177127,
               177128,
               177129,
               177130,
               177131,
               177132,
               177133]
    threads: "8"
    # bbduk parameters
    bbduk-params: "ktrim=r k=23 mink=11 hdist=1 tpe tbo qtrim=rl trimq=15"
    # path to a fasta file with the adapter sequences to be used for clipping
    bbduk-fasta: "/nfs/scistore16/itgrp/jelbers/bin/bbmap-38.90/resources/adapters.fa"
    # path to a fasta file with the adapter sequences to be used for clipping
    reference-fasta: "/nfs/scistore14/rcsb/pub/biogrp/ReferenceData/c.elegans/WBcel235-ce11/ensembl/Caenorhabditis_elegans.WBcel235.dna.toplevel.fa.gz"

### cluster settings

cluster.json

    {
        "__default__":
        {
            "account": "rehart",
            "time": "3:00:0",
            "mem": "64G",
            "job-name": "{rule}",
        },

        "bwa-mem2":
        {
            "account": "rerhart",
            "time": "3:00:0",
            "mem": "128G",
            "job-name" : "{rule}",
        },

        "octopus":
        {
            "account": "rerhart",
            "time": "05:00:0",
            "mem": "24G",
            "job-name" : "{rule}",
        }
    }

### snakemake Snakefile for pipeline

Snakefile-bwa-octopus-split-samtools

    # specify name of external config-file
    configfile: "config2.yaml"

    # import custom global parameter from config-file
    DATASETS = config["datasets"]
    THREADS = config["threads"]
    scattergather:
        split=28

    # list of rules which are not deployed to slurm
    localrules: all, fastqc, clean, adapters_fasta, rm_duplicates, mapping, make_test_bed, make_bed, snps


    # final target rule to produce all sub targets
    rule all:
        input:
            fastqc = "logs/fastqc.done",
            mapping = "logs/bwa-mem2.done",
            rm_duplicates = "logs/picard.done",
            snps = "logs/octopus.done"

    # step 1: check read quality of raw reads
    rule fastqc_check:
        input:
            mate1 = "raw/{sample}.mate1.fastq.gz",
            mate2 = "raw/{sample}.mate2.fastq.gz"
        output:
            mate1 = "qc/fastqc_raw/{sample}.mate1_fastqc.html",
            mate2 = "qc/fastqc_raw/{sample}.mate2_fastqc.html"
        params:
            threads = THREADS,
            dir = "qc/fastqc_raw"
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load fastqc/0.11.7
            fastqc --outdir {params.dir} --thread {params.threads} {input.mate1} {input.mate2}  > /dev/null 2> /dev/null
            '''


    # step 2: quality and adapter trimming of reads with bbduk
    rule adapters_fasta:
        input: config["bbduk-fasta"]
        output: "auxData/bbduk-fasta.fa"
        shell:
            '''
            cp {input} {output}
            '''

    rule bbduk:
        input:
            mate1 = "raw/{sample}.mate1.fastq.gz",
            mate2 = "raw/{sample}.mate2.fastq.gz",
            adapter = "auxData/bbduk-fasta.fa"
        output:
            mate1 = "fastq/{sample}-trimmed.mate1.fastq.gz",
            mate2 = "fastq/{sample}-trimmed.mate2.fastq.gz"
        log:
            err = "logs/bbduk/{sample}.log"
        params:
            threads = THREADS,
            params = config["bbduk-params"]       
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load bbtools/38.82
            module load bcftools/1.13
            bbduk.sh threads={params.threads} in1={input.mate1} in2={input.mate2} out1={output.mate1} out2={output.mate2} ref={input.adapter} {params.params} > {log.err} 2>&1
            '''

    # step 3: check read quality of trimmed reads
    rule fastqc_recheck:
        input:
            mate1 = "fastq/{sample}-trimmed.mate1.fastq.gz",
            mate2 = "fastq/{sample}-trimmed.mate2.fastq.gz"
        output:
            mate1 = "qc/fastqc_trimmed/{sample}-trimmed.mate1_fastqc.html",
            mate2 = "qc/fastqc_trimmed/{sample}-trimmed.mate2_fastqc.html"
        params:
            threads = THREADS
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load fastqc/0.11.7
            fastqc --outdir qc/fastqc_trimmed/ --thread {params.threads} {input.mate1} {input.mate2}  > /dev/null 2> /dev/null
            '''

    rule fastqc:
        input:
            fastq1 = expand("qc/fastqc_trimmed/{dataset}-trimmed.mate1_fastqc.html", dataset=DATASETS),
            fastq2 = expand("qc/fastqc_trimmed/{dataset}-trimmed.mate2_fastqc.html", dataset=DATASETS),
            fastq3 = expand("qc/fastqc_raw/{dataset}.mate1_fastqc.html", dataset=DATASETS),
            fastq4 = expand("qc/fastqc_raw/{dataset}.mate2_fastqc.html", dataset=DATASETS)
        output: "logs/fastqc.done"
        shell:
            '''
            touch {output}
            '''

    # step 4: map reads to reference genome
    rule reference_fasta:
        input: config["reference-fasta"]
        output: 
            reference = "auxData/reference-fasta.fa",
            index = "auxData/reference-fasta.fa.fai"
        log: "logs/reference/bwa-mem2-index.log"
        shell:
            '''
            # get C elegans reference from Fabian (from Ensembl presumably)
            # see config.yaml on path to reference
            module load seqtk/20210818
            module load samtools/1.13
            module load bwa-mem2/2.2.1
            zcat {input} |seqtk seq -UC > {output.reference}
            bwa-mem2 index -p {output.reference} {output.reference} > {log} 2>&1
            samtools faidx {output.reference}
            '''

    rule bwa_mem2:
        input:
            mate1 = "fastq/{sample}-trimmed.mate1.fastq.gz",
            mate2 = "fastq/{sample}-trimmed.mate2.fastq.gz",
            reference = "auxData/reference-fasta.fa"
        output:
            bam = "bam/{sample}.bam",
            index = "bam/{sample}.bam.bai"
        log:
            err = "logs/bwa-mem2/{sample}.log"
        params:
            threads = THREADS
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load samtools/1.13
            module load bwa-mem2/2.2.1
            bwa-mem2 mem -t {params.threads} \
            -R "@RG\tID:{wildcards.sample}\tSM:{wildcards.sample}\tPL:ILLUMINA\tPU:barcode" \
            {input.reference} {input.mate1} {input.mate2} 2> {log.err}| samtools sort -@ {params.threads} -o {output.bam} -  2>logs/samtools/{wildcards.sample}.log
            samtools index -@ {params.threads} {output.bam}
            '''

    rule mapping:
        input:
            interleaved = expand("bam/{dataset}.bam", dataset=DATASETS)
        output: "logs/bwa-mem2.done"
        shell:
            '''
            touch {output}
            '''

    # step 5: mark pcr duplicates and optical duplicates for NovaSeq (--OPTICAL_DUPLICATE_PIXEL_DISTANCE=2500)
    rule picard:
        input:
            bam = "bam/{sample}.bam",
            index = "bam/{sample}.bam.bai"
        output:
            bam1 = temp("bam/{sample}2.bam"),
            bam2 = "bam/{sample}-md.bam",
            index = "bam/{sample}-md.bai",
            metrics = "bam/{sample}-metrics.txt"
        log:
            err = "logs/picard/{sample}.log"
        shell:
            '''
            module load samtools/1.13
            samtools reheader \
            <(samtools view -H {input.bam} |\
            perl -pe "s/-R \@RG\tID:{wildcards.sample}\tSM:{wildcards.sample}\tPL:ILLUMINA\tPU:barcode/-R \@RG ID:{wildcards.sample} SM:{wildcards.sample} PL:ILLUMINA PU:barcode/g"|\
            perl -pe "s/PP:illumina/PP:bwa-mem2/g") \
            {input.bam} > {output.bam1}

            module unload samtools/1.13
            module load picard/2.25.6
            java -jar $PICARD MarkDuplicates \
            --METRICS_FILE {output.metrics} \
            --INPUT {output.bam1} \
            --OUTPUT {output.bam2} \
            --ASSUME_SORT_ORDER coordinate \
            --OPTICAL_DUPLICATE_PIXEL_DISTANCE 2500 \
            --CREATE_INDEX true > {log.err} 2>&1
            '''

    rule rm_duplicates:
        input:
            bam = expand("bam/{dataset}-md.bam", dataset=DATASETS)
        output: "logs/picard.done"
        shell:
            '''
            touch {output}
            '''

    # step 6 call SNPs
    rule make_test_bed:
        input:
            "auxData/reference-fasta.fa.fai"
        output:
            "vcf/test.bed"
        log:
            "logs/octopus/test.bed.err"
        shell:
            '''
            module load bedtools/2.29.2
            bedtools makewindows -g {input} -n 4 > {output} 2> {log}
            '''

    rule make_bed:
        input:
            "vcf/test.bed"
        output:
            scatter.split("vcf/{{sample}}_{scatteritem}.bed")
        run:
            shell("touch {output}")
            for i in range(1, 29):
                shell("sed -n {i}p {input} > vcf/{wildcards.sample}_{i}-of-28.bed")

    rule octopus:
        input:
            bed = "vcf/{sample}_{scatteritem}.bed",
            bam = "bam/{sample}-md.bam",
            reference = "auxData/reference-fasta.fa"
        output:
            vcf1 = temp("vcf/{sample}_{scatteritem}.vcf"),
            vcf2 = "vcf/{sample}_{scatteritem}.vcf.gz",
            index = "vcf/{sample}_{scatteritem}.vcf.gz.tbi"
        log:
            err = "logs/octopus/{sample}_{scatteritem}.log"
        params:
            threads = THREADS
        shell:
            '''
            unset SLURM_EXPORT_ENV
            module load octopus/0.7.4
            module load python/3.8.5
            octopus --regions-file {input.bed} --temp-directory-prefix vcf/{wildcards.sample}_{wildcards.scatteritem}.temp --reference {input.reference} --reads {input.bam} --threads {params.threads} -o {output.vcf1} > {log.err} 2>&1
            module load bcftools/1.13
            bgzip -@{params.threads} {output.vcf1} -c > {output.vcf2}
            tabix -p vcf {output.vcf2}
            '''

    rule gather_vcf:
        input:
            gather.split("vcf/{{sample}}_{scatteritem}.vcf.gz")
        output:
            "vcf/{sample}.vcf"
        params:
            threads = THREADS
        shell:
            '''
            module load bcftools/1.13
            bcftools concat --threads {params.threads} {input} > {output}
            '''

    rule snps:
        input:
            vcf = expand("vcf/{sample}.vcf", sample=DATASETS)
        output: "logs/octopus.done"
        shell:
            '''
            touch {output}
            '''

    # rule to remove all workflow intermediate and results files, but the logs
    rule clean:
        shell:
            '''
            rm -rf vcf/        
            rm -rf bam/        
            rm -rf fastq/
            rm -rf qc/
            rm -rf auxData/
            rm -rf logs/*.done
            '''

### run snakemake pipeline

#### activate snakemake-6.3.0 conda environment
 
    conda activate snakemake-6.3.0

#### make the DAG graph to view what will happen

    snakemake --snakefile Snakefile-bwa-octopus-split --dryrun  --dag all | dot -Tsvg > dag.svg

#### actually run the pipeline

    snakemake --snakefile Snakefile-bwa-octopus-split-samtools --printshellcmds \
    --latency-wait 60 --local-cores 1 --cores all --cluster-config cluster.json \
    --cluster "sbatch --export=NONE --no-requeue --job-name {cluster.job-name} --mem={cluster.mem} --time={cluster.time} --cores={THREADS} --constraint=avx2" all > Snakefile-bwa-octopus-split-samtools.log 2>&1 &

### generate coverage plots

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.1/qc
    module load picard/2.25.6

    for i in `ls ../bam/*-md.bam`
    do
        j=`echo "${i}"|perl -pe "s/\.\.\/bam\///g"`
        java -Xmx60g -jar $PICARD CollectWgsMetrics \
        I=../bam/${j} \
        O=${j}_wgs_metrics.txt \
        R=../auxData/reference-fasta.fa \
        INCLUDE_BQ_HISTOGRAM=false \
        READ_LENGTH=150 > ${j}.wg_metrics.log 2>&1
    done &

### run multiqc to get trimming as well as coverage plots/metrics

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.1/qc

    conda activate multiqc (using multiqc 1.11)

    multiqc -f -ip . -n DA0033-08_Nov_2021-analysis.1

### Do something with the octopus called SNPs

#### determine what sites are not mutations but differences from the reference genome

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.1/vcf

    ## merge the mutant crossed to the background strains
    module load bcftools/1.14

    bcftools merge \
    --no-index \
    177115.vcf \
    177116.vcf \
    177117.vcf \
    177118.vcf \
    177119.vcf \
    177124.vcf \
    177125.vcf \
    177126.vcf \
    177127.vcf \
    177123.vcf \
    177128.vcf \
    177129.vcf | \
    bcftools view -m2 -M2 --apply-filters "PASS" --types snps | \
    perl -pe "s/^(\S+)\t(\S+)\t\./\1\t\2\t\1_\2/g" > 177120-crosses.pass.snps.merged.vcf

    ### bcftools view -m2 -M2 --apply-filters "PASS" --types snps makes sure we bi-alleleic SNPs that PASS octopus filters
    ### perl -pe "s/^(\S+)\t(\S+)\t\./\1\t\2\t\1_\2/g" this part makes sure that there is a SNPid in the form chromosome_position in the third
    ### column of the VCF file so that we can filter them out later with plink's genoytpe frequency

    ## use PLINK to get the genotype frequencies

    module load plink/1.90
    plink --freqx --vcf 177120-crosses.pass.snps.merged.vcf --allow-extra-chr --out 177120-crosses.pass.snps.merged
    grep -P "\t4\t|\t5\t|\t6\t|\t7\t|\t8\t|\t9\t|\t10\t|\t11\t|\t12\t" 177120-crosses.pass.snps.merged.frqx > 177120-crosses.pass.snps.merged.frqx.diff.from.ref

    ### this part grep -P "\t4\t|\t5\t|\t6\t|\t7\t|\t8\t|\t9\t|\t10\t|\t11\t|\t12\t" 177120-crosses.pass.snps.merged.frqx makes sure that we only keep sites where the
    ### genotype is the same across at least 4 (1/3) of the samples which would likely mean that difference from the reference genome
    ### and not a true SNP for a mutation caused by EMS


    # get differences from reference and not real SNPs
    cat <(grep "^#" 177120-crosses.pass.snps.merged.vcf) <(grep -f <(tail -n+2 177120-crosses.pass.snps.merged.frqx.diff.from.ref|cut -f 2) \
    177120-crosses.pass.snps.merged.vcf) > 177120-crosses.pass.snps.merged.not.real.snps.vcf


    ## now get the passing SNPs in each mutant crossed to strain Ax8200 (177120) except the differences from the reference genome (not reference strain)
    module load bedtools/2.30.0

    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129\n177120"`
    do
      bcftools view -m2 -M2 --apply-filters "PASS" --types snps --genotype ^miss ${i}.vcf | \
      bedtools intersect -v -a - -b 177120-crosses.pass.snps.merged.not.real.snps.vcf -header > ${i}.pass.snps.vcf
    done

    ## now remove the genotypes that are identical to the reference strain Ax8200 (177120) and 
    ## the mutant crossed to the reference strain (the grep -v "TP" part)
    ## also get rid of missing SNPs in the mutant crossed to the reference strain
    ## this gives two (I think) types of scenarios that could be associated with the phenotype:
    ## class 1 an EMS mutation where for example the genotype goes from C/T to C/C, common
    ## class 2 an EMS mutation where for example the genotype goes from C/T to C/G, super rare

    module load picard/2.25.6

    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do
      java -jar $PICARD GenotypeConcordance \
      CALL_VCF=${i}.pass.snps.vcf \
      O=${i} \
      TRUTH_VCF=177120.pass.snps.vcf \
      OUTPUT_VCF=TRUE > ${i}.genotype_concordance.log 2>&1
      zcat ${i}.genotype_concordance.vcf.gz|grep -v "TP"|grep -vP "\t.\/.\t" > ${i}.candidate.mutations.vcf
    done &


    ## for class II mutations, get the VCF files and GFF3 files based on Ensembl 46(?) annotations
    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do
      grep -v ".\/." ${i}.candidate.mutations.vcf > ${i}.candidate.mutations.classII.vcf
      bedtools intersect -a /nfs/scistore14/rcsb/pub/biogrp/ReferenceData/c.elegans/WBcel235-ce11/ensembl/Caenorhabditis_elegans.WBcel235.46.gff3.gz \
      -b ${i}.candidate.mutations.classII.vcf > ${i}.candidate.mutations.classII.gff3
    done


#### make the list of octopus SNPs even smaller to make sure these sites that agree with the same genotype as callvariants.sh

    module load bbtools/38.82
    module load samtools/1.14
    module load picard/2.25.6

    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do
      # run callvariants.sh
      callvariants.sh ref=../auxData/reference-fasta.fa in=../bam/${i}-md.bam out=${i}.callvariants.vcf ploidy=2 threads=8 \
      callindels=f > ${i}.callvariants.log 2>&1
      # get sites that agree exactly between octopus and callvariants.sh
      java -jar $PICARD GenotypeConcordance \
      CALL_VCF=${i}.callvariants.vcf \
      O=${i}.callvariants \
      TRUTH_SAMPLE=call \
      TRUTH_VCF=${i}.candidate.mutations.vcf \
      OUTPUT_VCF=TRUE > ${i}.callvariants.genotype_concordance.log 2>&1
      zcat ${i}.callvariants.genotype_concordance.vcf.gz|grep "TP\|^#" > ${i}.callvariants.candidate.mutations.vcf
    done &

    ## for class II mutations, get the VCF files and GFF3 files based on Ensembl 46(?) annotations
    ## difference here is these are the sites that agree between callvariants.sh and octopus
    module load bedtools/2.30.0

    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do
      bedtools intersect -a ${i}.candidate.mutations.classII.vcf -b ${i}.callvariants.candidate.mutations.vcf -header > ${i}.candidate.mutations.classII.callvariants.intersect.vcf
      bedtools intersect -a /nfs/scistore14/rcsb/pub/biogrp/ReferenceData/c.elegans/WBcel235-ce11/ensembl/Caenorhabditis_elegans.WBcel235.46.gff3.gz \
      -b ${i}.candidate.mutations.classII.callvariants.intersect.vcf > ${i}.candidate.mutations.classII.callvariants.intersect.gff3
    done

    zip DA0033-08_Nov_2021-analysis.1-candidates-as-of-25Nov2021.zip *candidate.mutations.classII.callvariants.intersect*

---

## Analysis.2 Call SNPs with mimodd

Accessed around 9 Dec 2021
MiModD[https://mimodd.readthedocs.io/en/latest/index.html]

#### first remove marked PCR and optical duplicates from the background strain

    samtools view -h -@12 -F 1024 ../../analysis.1/bam/177120-md.bam \
    | samtools view -@12 -Sb > 177120.no.duplicates.bam

### call snps and indels

varcall.sh

    #!/bin/bash
    #
    #----------------------------------------------------------------
    # running a multiple independent jobs
    #----------------------------------------------------------------
    #


    #  Defining options for slurm how to run
    #----------------------------------------------------------------
    #
    #SBATCH --job-name=varcall
    #SBATCH --output=varcall.sh.log
    #
    #Number of CPU cores to use within one node
    #SBATCH -c 12
    #
    #Define the number of hours the job should run. 
    #Maximum runtime is limited to 10 days, ie. 240 hours
    #SBATCH --time=48:00:00
    #
    #Define the amount of RAM used by your job in GigaBytes
    #In shared memory applications this is shared among multiple CPUs
    #SBATCH --mem=64G
    #
    #Do not requeue the job in the case it fails.
    #SBATCH --no-requeue
    #
    #Do not export the local environment to the compute nodes
    #SBATCH --export=NONE
    unset SLURM_EXPORT_ENV

    # load the respective software module(s) you intend to use
    #----------------------------------------------------------------
    module load R/4.1.2
    module load python/3.7
    module load samtools/1.14
    export PATH="/nfs/scistore16/itgrp/jelbers/.local/lib/python3.7/site-packages/MiModD/bin:$PATH"
    python3 -m MiModD.config --tmpfiles /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.2/ --snpeff /nfs/scistore16/itgrp/jelbers/bin/snpEff_v4_3t/snpEff  -t 12 -m 64 --no-use-galaxy-index-files


    # define sequence of jobs to run as you would do in a BASH script
    # use variable $SLURM_ARRAY_TASK_ID to address individual behaviour
    # in different iteration of the script execution
    #----------------------------------------------------------------
    # makes a map of the reads
    # like
    # SLURM_ARRAY_TASK_ID  sequence_name
    # 1			123456
    # 2			123457
    # 3			123458
    # and runs the array job using the sequence name

    i=`grep -P "^${SLURM_ARRAY_TASK_ID}\t" <(paste <(seq 1 12) \
    <(echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"))|cut -f 2`

    #Step 1 - remove marked duplicates
    samtools view -h -@12 -F 1024 ../analysis.1/bam/${i}-md.bam |samtools view -@12 -Sb - > ${i}.no.duplicates.bam

    #Step 2 - merge bwa-mem2 bams
    samtools merge -f -@12 ${i}-177120.bam 177120.no.duplicates.bam ${i}.no.duplicates.bam

    #Step 3 - combined variant calling for both samples
    mimodd varcall ../analysis.1/auxData/reference-fasta.fa ${i}-177120.bam \
    -o ${i}-177120.bam.bcf > ${i}-177120.bam.bcf.log 2>&1
    ######

### run the array job

    sbatch --array=1-12 varcall.sh

### convert bcf to vcf
#### there might be some other variant filtering done by mimodd that I am not aware of in this step

    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do
      mimodd varextract ${i}-177120.bam.bcf -o ${i}-177120.bam.bcf.vcf &
    done

### do SVD analysis

    mkdir -p svd-mimodd-no11
    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do
        # first keep only sites where the F2 pool is 1/1 and the background strain is 0/0 for a SNP or indel genotype
        mimodd vcf-filter ${i}-177120.bam.bcf.vcf --sample ${i} 177120 --gt 1/1 0/0 -o ${i}-177120.bam.bcf.vcf2
        
        # then run SVD analysis
        mimodd map SVD ${i}-177120.bam.bcf.vcf2 -o svd-mimodd-no11/${i}-177120.bam.bcf.vcf2.svd.txt \
        -p svd-mimodd-no11/${i}-177120.bam.bcf.vcf2.svd.pdf
    done

### do VAF analysis

    mkdir -p vaf-mimodd
    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do
        mimodd map VAF --loess-span 0 -m ${i} -r 177120 ${i}-177120.bam.bcf.vcf \
        -o vaf-mimodd/${i}-177120.bam.bcf.vcf.vaf.txt \
        -p vaf-mimodd/${i}-177120.bam.bcf.vcf.vaf.pdf
    done
 
### do VAC analysis

    mkdir -p vac-mimodd
    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do 
        # first filter on 10x coverage
        mimodd vcf-filter ${i}-177120.bam.bcf.vcf -s ${i} 177120 --dp 10 10 \
        -o vac-mimodd/${i}-177120.bam.bcf.dp10.vcf
        
        # then do VAC analysis on at 10x coverage sites
        mimodd map VAC --loess-span 0 -m ${i} -c 177120 vac-mimodd/${i}-177120.bam.bcf.dp10.vcf \
        -o vac-mimodd/${i}-177120.bam.bcf.dp10.vcf.vac.txt -p vac-mimodd/${i}-177120.bam.bcf.dp10.vcf.vac.pdf
    done

### copy trimmed FASTQ, BAM, BCF, VCF, TXT, PDF files to biogrp

    cd /fs3/group/biogrp/deBono/DA0033/08_Nov_2021
    rsync --stats --progress --archive /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.1/fastq/ analysis.1/fastq/ -n

    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"`
    do
        rsync --stats --progress --archive --append-verify /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.2/${i}.no.duplicates.bam analysis.2/${i}.no.duplicates.bam
        rsync --stats --progress --archive --append-verify /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.2/${i}-177120.bam analysis.2/${i}-177120.bam
        rsync --stats --progress --archive --append-verify /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.2/${i}-177120.bam.bcf analysis.2/${i}-177120.bam.bcf
        rsync --stats --progress --archive --append-verify /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.2/${i}-177120.bam.bcf.vcf analysis.2/${i}-177120.bam.bcf.vcf
    done

    rsync --stats --progress --archive --append-verify /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.2/vaf-mimodd/ analysis.2/vaf-mimodd/
    rsync --stats --progress --archive --append-verify /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.2/vac-mimodd/ analysis.2/vac-mimodd/

---
 
## Analysis.3 Call SNPs with callvariants.sh

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.3

    module load bbtools/38.82
    module load samtools/1.14
    module load R/4.1.2
    module load bedtools/2.30.0
    export PATH="/nfs/scistore16/itgrp/jelbers/.local/lib/python3.7/site-packages/MiModD/bin:$PATH"

    for SLURM_ARRAY_TASK_ID in `seq 1 12`
    do
    i=`grep -P "^${SLURM_ARRAY_TASK_ID}\t" <(paste <(seq 1 12) \
    <(echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"))|cut -f 2`

    mkdir -p ${i}
    cd ${i}
    echo -e "../../analysis.1/bam/${i}-md.bam\n../../analysis.1/bam/177120-md.bam" > bams${SLURM_ARRAY_TASK_ID}

    ~/bin/bbmap-38.94/callvariants.sh ref=../../analysis.1/auxData/reference-fasta.fa \
    list=bams${SLURM_ARRAY_TASK_ID} out=${i}-177120.callvariants.vcf ploidy=2 multisample=t > ${i}-177120.callvariants.log 2>&1
    cd ..
    done &

    for i in `echo -e "177115\n177116\n177117\n177118\n177119\n177124\n177125\n177126\n177127\n177123\n177128\n177129"|head -n 7`
    do
      # Retain only variant sites at which the bc_mutant sample is homozygous mutant
      # (genotype 1/1), but at which the parent sample appears to be homozygous wt (0/0),
      # i.e., eliminate variants that may have been inherited from the parent strain.
      mimodd vcf-filter ${i}/${i}-177120.callvariants.vcf --sample ${i}-md 177120-md --gt 1/1 0/0 -o ${i}-177120.callvariants2.vcf

      # Generate a plot and a tabular report of the distribution of all variant sites
      # that passed the previous step.
      mimodd map SVD ${i}-177120.callvariants2.vcf -o ${i}-177120.callvariants2.vcf.txt -p ${i}-177120.callvariants2.vcf.pdf

      # get the genes,mRNA,exons these sites appear in
      bedtools intersect -a /nfs/scistore14/rcsb/pub/biogrp/ReferenceData/c.elegans/WBcel235-ce11/ensembl/Caenorhabditis_elegans.WBcel235.46.gff3.gz \
      -b ${i}-177120.callvariants2.vcf > ${i}-177120.callvariants2.gff3
    done

    zip DA0033-08_Nov_2021-analysis.3-candidates-as-of-26Nov2021.zip *.vcf *.pdf *.gff3

---

## Analysis.4 Call SNPs with octopus with --refcall

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.4/vcf

### make 448 region files so octopus runs on 448 split regions
    module load bedtools/2.30.0
    bedtools makewindows -g ../../analysis.1/auxData/reference-fasta.fa.fai -n 64 > test.bed

    for i in `seq 1 448`
    do
      sed -n ${i}p test.bed > 177120_${i}-of-448.bed
    done

### octopus array job for 177120

octopus.sh

    #!/bin/bash
    #
    #----------------------------------------------------------------
    # running a multiple independent jobs
    #----------------------------------------------------------------
    #
    #  Defining options for slurm how to run
    #----------------------------------------------------------------
    #
    #SBATCH --job-name=octopus
    #SBATCH --output=octopus.sh.log
    #
    #Number of CPU cores to use within one node
    #SBATCH -c 1
    #
    #Define the number of hours the job should run. 
    #Maximum runtime is limited to 10 days, ie. 240 hours
    #SBATCH --time=24:00:00
    #
    #Define the amount of RAM used by your job in GigaBytes
    #In shared memory applications this is shared among multiple CPUs
    #SBATCH --mem=24G
    #
    #Do not requeue the job in the case it fails.
    #SBATCH --no-requeue
    #
    #SBATCH --constraint=avx2
    #Do not export the local environment to the compute nodes
    #SBATCH --export=NONE
    unset SLURM_EXPORT_ENV

    # load the respective software module(s) you intend to use
    #----------------------------------------------------------------
    module load octopus/0.7.4
    octopus --sequence-error-model PCR.NOVASEQ --refcall POSITIONAL \
    --regions-file 177120_${SLURM_ARRAY_TASK_ID}-of-448.bed --temp-directory-prefix 177120_${SLURM_ARRAY_TASK_ID}.temp \
    --reference ../auxData/reference-fasta.fa --reads ../../analysis.2/177120.no.duplicates.bam \
    --threads 1 -o 177120_${SLURM_ARRAY_TASK_ID}-of-448.vcf > 177120_${SLURM_ARRAY_TASK_ID}-of-448.log 2>&1
    module load bcftools/1.14
    ######

### run the array job

    sbatch --array=1-448 octopus.sh

### compress the 448 files, then index them

    module load bcftools/1.14

    for SLURM_ARRAY_TASK_ID in `seq 1 448`
    do
      bgzip -@20 177120_${SLURM_ARRAY_TASK_ID}-of-448.vcf -c > 177120_${SLURM_ARRAY_TASK_ID}-of-448.vcf.gz
      tabix -f -p vcf 177120_${SLURM_ARRAY_TASK_ID}-of-448.vcf.gz
    done &

### concatenate the 448 files into one then only get sites that "PASS" octopus filters

    bcftools concat --threads 20 `ls -rt 177120_*-of-448.vcf.gz` > 177120.octopus.refcall.vcf 2>177120.octopus.refcall.concat.log &
    bcftools view --apply-filters "PASS" 177120.octopus.refcall.vcf > 177120.octopus.refcall.pass.vcf &

### work on sample 177115

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.4/vcf

### octopus array job script for 177115

octopus2.sh

    #!/bin/bash
    #
    #----------------------------------------------------------------
    # running a multiple independent jobs
    #----------------------------------------------------------------
    #
    #  Defining options for slurm how to run
    #----------------------------------------------------------------
    #
    #SBATCH --job-name=octopus2
    #SBATCH --output=octopus2.sh.log
    #
    #Number of CPU cores to use within one node
    #SBATCH -c 1
    #
    #Define the number of hours the job should run. 
    #Maximum runtime is limited to 10 days, ie. 240 hours
    #SBATCH --time=24:00:00
    #
    #Define the amount of RAM used by your job in GigaBytes
    #In shared memory applications this is shared among multiple CPUs
    #SBATCH --mem=24G
    #
    #Do not requeue the job in the case it fails.
    #SBATCH --no-requeue
    #
    #SBATCH --constraint=avx2
    #Do not export the local environment to the compute nodes
    #SBATCH --export=NONE
    unset SLURM_EXPORT_ENV

    # load the respective software module(s) you intend to use
    #----------------------------------------------------------------
    module load octopus/0.7.4
    octopus --sequence-error-model PCR.NOVASEQ --refcall POSITIONAL \
    --regions-file 177120_${SLURM_ARRAY_TASK_ID}-of-448.bed --temp-directory-prefix 177115_${SLURM_ARRAY_TASK_ID}.temp \
    --reference ../auxData/reference-fasta.fa --reads ../../analysis.2/177115.no.duplicates.bam \
    --threads 1 -o 177115_${SLURM_ARRAY_TASK_ID}-of-448.vcf > 177115_${SLURM_ARRAY_TASK_ID}-of-448.log 2>&1
    ######
    sbatch --array=1-448 octopus2.sh

### compress then index the 448 files

    module load bcftools/1.14

    for SLURM_ARRAY_TASK_ID in `seq 1 448`
    do
      bgzip -@20 177115_${SLURM_ARRAY_TASK_ID}-of-448.vcf -c > 177115_${SLURM_ARRAY_TASK_ID}-of-448.vcf.gz
      tabix -f -p vcf 177115_${SLURM_ARRAY_TASK_ID}-of-448.vcf.gz
    done &

### concatenate the 448 files then get only the sites that "PASS" octopus quality filters

    bcftools concat --threads 20 `ls -rt 177115_*-of-448.vcf.gz` 2>177115.octopus.refcall.concat.log \
    | bcftools view --threads 20 --apply-filters "PASS" > 177115.octopus.refcall.pass.vcf


### compress 177115 and 177120 sample variants then index them for easier merging

    bgzip -@20 177115.octopus.refcall.pass.vcf -c > 177115.octopus.refcall.pass.vcf.gz
    tabix -p vcf 177115.octopus.refcall.pass.vcf.gz

    bgzip -@20 177120.octopus.refcall.pass.vcf -c > 177120.octopus.refcall.pass.vcf.gz
    tabix -p vcf 177120.octopus.refcall.pass.vcf.gz

### merge 177115 and 177120 sites that pass filters then keep only merged sites that have no missing genotypes

    bcftools merge --threads 20 177115.octopus.refcall.pass.vcf.gz 177120.octopus.refcall.pass.vcf.gz \
    | bcftools view --threads 20 --genotype ^miss \
    > 177115-177120.octopus.refcall.pass.noMiss.vcf &

### remove sites where both 177115 and 177120 are 0/0 or the same as the reference genome

    grep -Pv "0\|0\S+\t0\|0\S+$" 177115-177120.octopus.refcall.pass.noMiss.vcf > 177115-177120.octopus.refcall.pass.noMiss.no0000.vcf &

### get the sites that have different genotypes between 177115 (F2 pool) and 177120 (background strain)

    module load picard/2.25.6
    java -jar $PICARD GenotypeConcordance \
    CALL_VCF=177115-177120.octopus.refcall.pass.noMiss.no0000.vcf \
    CALL_SAMPLE=177115 \
    O=177115-177120.octopus.refcall.pass.noMiss.no0000 \
    TRUTH_VCF=177115-177120.octopus.refcall.pass.noMiss.no0000.vcf \
    TRUTH_SAMPLE=177120 \
    OUTPUT_VCF=TRUE

    module load bedtools/2.30.0
    bedtools intersect \
    -a 177115-177120.octopus.refcall.pass.noMiss.no0000.vcf \
    -b <(zcat 177115-177120.octopus.refcall.pass.noMiss.no0000.genotype_concordance.vcf.gz|grep -v "TP\|\.\/\.") \
    -header \
    | perl -pe "s/(\d+)\|(\d+)/\1\/\2/g" > 177115-177120.octopus.refcall.pass.noMiss.n0000.onlyDiffGenotypes.vcf &

### run mimodd svd mapping
    module load R/4.1.2
    module load python/3.7
    module load samtools/1.14
    export PATH="/nfs/scistore16/itgrp/jelbers/.local/lib/python3.7/site-packages/MiModD/bin:$PATH"
    python3 -m MiModD.config --tmpfiles /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.4/ --snpeff /nfs/scistore16/itgrp/jelbers/bin/snpEff_v4_3t/snpEff  -t 12 -m 64 --no-use-galaxy-index-files

    mimodd map SVD 177115-177120.octopus.refcall.pass.noMiss.n0000.onlyDiffGenotypes.vcf -o 177115-177120.octopus.refcall.pass.noMiss.n0000.onlyDiffGenotypes.svd.vaf.txt

#### doesn't seem to be any better than running mimodd with default pipeline (except using bwa-mem2 instead of SNAP short-read aligner)

---

## Analysis.5 Call SNPs with mimodd but for 177130-177133 samples

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.5
    module load samtools/1.14

### remove marked PCR and optical duplicates for the two background strains
    samtools view -h -@12 -F 1024 ../analysis.1/bam/177106-md.bam |samtools view -@20 -Sb > 177106.no.duplicates.bam
    samtools view -h -@12 -F 1024 ../analysis.1/bam/177121-md.bam |samtools view -@20 -Sb > 177121.no.duplicates.bam

### mimodd for samples 177130-177132

mimodd.sh

    #!/bin/bash
    #
    #----------------------------------------------------------------
    # running a multiple independent jobs
    #----------------------------------------------------------------
    #


    #  Defining options for slurm how to run
    #----------------------------------------------------------------
    #
    #SBATCH --job-name=mimodd1_${SLURM_ARRAY_TASK_ID}
    #SBATCH --output=mimodd.sh.log
    #
    #Number of CPU cores to use within one node
    #SBATCH -c 12
    #
    #Define the number of hours the job should run. 
    #Maximum runtime is limited to 10 days, ie. 240 hours
    #SBATCH --time=48:00:00
    #
    #Define the amount of RAM used by your job in GigaBytes
    #In shared memory applications this is shared among multiple CPUs
    #SBATCH --mem=64G
    #
    #Do not requeue the job in the case it fails.
    #SBATCH --no-requeue
    #
    #Do not export the local environment to the compute nodes
    #SBATCH --export=NONE
    unset SLURM_EXPORT_ENV

    # load the respective software module(s) you intend to use
    #----------------------------------------------------------------
    module load R/4.1.2
    module load python/3.7
    module load samtools/1.14
    export PATH="/nfs/scistore16/itgrp/jelbers/.local/lib/python3.7/site-packages/MiModD/bin:$PATH"
    python3 -m MiModD.config --tmpfiles /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.5/ --snpeff /nfs/scistore16/itgrp/jelbers/bin/snpEff_v4_3t/snpEff  -t 12 -m 64 --no-use-galaxy-index-files


    # define sequence of jobs to run as you would do in a BASH script
    # use variable $SLURM_ARRAY_TASK_ID to address individual behaviour
    # in different iteration of the script execution
    #----------------------------------------------------------------
    # makes a map of the reads
    # like
    # SLURM_ARRAY_TASK_ID  sequence_name
    # 1			123456
    # 2			123457
    # 3			123458
    # and runs the array job using the sequence name

    i=`grep -P "^${SLURM_ARRAY_TASK_ID}\t" <(paste <(seq 1 3) \
    <(echo -e "177130\n177131\n177132"))|cut -f 2`

    #Step 1 - remove marked duplicates
    samtools view -h -@12 -F 1024 ../analysis.1/bam/${i}-md.bam |samtools view -@12 -Sb - > ${i}.no.duplicates.bam

    #Step 2 - merge bwa-mem2 bams
    samtools merge -f -@12 ${i}-177106.bam 177106.no.duplicates.bam ${i}.no.duplicates.bam

    #Step 3 - combined variant calling for both samples
    mimodd varcall ../analysis.1/auxData/reference-fasta.fa ${i}-177106.bam -o ${i}-177106.bam.bcf > ${i}-177106.bam.bcf.log 2>&1

    #Step 4 - extract variants
    mimodd varextract ${i}-177106.bam.bcf -o ${i}-177106.bam.bcf.vcf

    mkdir -p vaf-mimodd
    #Step 5 - VAF 
    mimodd map VAF --loess-span 0 -m ${i} -r 177106 ${i}-177106.bam.bcf.vcf -o vaf-mimodd/${i}-177106.bam.bcf.vcf.vaf.txt -p vaf-mimodd/${i}-177106.bam.bcf.vcf.vaf.pdf

    mkdir -p vac-mimodd
    #Step 6 - VAC
    mimodd vcf-filter ${i}-177106.bam.bcf.vcf -s ${i} 177106 --dp 10 10 -o vac-mimodd/${i}-177106.bam.bcf.dp10.vcf
    mimodd map VAC --loess-span 0 -m ${i} -c 177106 vac-mimodd/${i}-177106.bam.bcf.dp10.vcf -o vac-mimodd/${i}-177106.bam.bcf.dp10.vcf.vac.txt -p vac-mimodd/${i}-177106.bam.bcf.dp10.vcf.vac.pdf
    ######

### run the mimodd array job

    sbatch --array=1-3 mimodd.sh

### script for mimodd on 177133 sample

mimodd2.sh

    #!/bin/bash
    #
    #----------------------------------------------------------------
    # running a multiple independent jobs
    #----------------------------------------------------------------
    #


    #  Defining options for slurm how to run
    #----------------------------------------------------------------
    #
    #SBATCH --job-name=mimodd2_${SLURM_ARRAY_TASK_ID}
    #SBATCH --output=mimodd2.sh.log
    #
    #Number of CPU cores to use within one node
    #SBATCH -c 12
    #
    #Define the number of hours the job should run. 
    #Maximum runtime is limited to 10 days, ie. 240 hours
    #SBATCH --time=48:00:00
    #
    #Define the amount of RAM used by your job in GigaBytes
    #In shared memory applications this is shared among multiple CPUs
    #SBATCH --mem=64G
    #
    #Do not requeue the job in the case it fails.
    #SBATCH --no-requeue
    #
    #Do not export the local environment to the compute nodes
    #SBATCH --export=NONE
    unset SLURM_EXPORT_ENV

    # load the respective software module(s) you intend to use
    #----------------------------------------------------------------
    module load R/4.1.2
    module load python/3.7
    module load samtools/1.14
    export PATH="/nfs/scistore16/itgrp/jelbers/.local/lib/python3.7/site-packages/MiModD/bin:$PATH"
    python3 -m MiModD.config --tmpfiles /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.5/ --snpeff /nfs/scistore16/itgrp/jelbers/bin/snpEff_v4_3t/snpEff  -t 12 -m 64 --no-use-galaxy-index-files


    # define sequence of jobs to run as you would do in a BASH script
    # use variable $SLURM_ARRAY_TASK_ID to address individual behaviour
    # in different iteration of the script execution
    #----------------------------------------------------------------
    # makes a map of the reads
    # like
    # SLURM_ARRAY_TASK_ID  sequence_name
    # 1			123456
    # 2			123457
    # 3			123458
    # and runs the array job using the sequence name

    i=`grep -P "^${SLURM_ARRAY_TASK_ID}\t" <(paste <(seq 1 1) \
    <(echo -e "177133"))|cut -f 2`

    #Step 1 - remove marked duplicates
    samtools view -h -@12 -F 1024 ../analysis.1/bam/${i}-md.bam |samtools view -@12 -Sb - > ${i}.no.duplicates.bam

    #Step 2 - merge bwa-mem2 bams
    samtools merge -f -@12 ${i}-177121.bam 177121.no.duplicates.bam ${i}.no.duplicates.bam

    #Step 3 - combined variant calling for both samples
    mimodd varcall ../analysis.1/auxData/reference-fasta.fa ${i}-177121.bam -o ${i}-177121.bam.bcf > ${i}-177121.bam.bcf.log 2>&1

    #Step 4 - extract variants
    mimodd varextract ${i}-177121.bam.bcf -o ${i}-177121.bam.bcf.vcf

    mkdir -p vaf-mimodd
    #Step 5 - VAF 
    mimodd map VAF --loess-span 0 -m ${i} -r 177121 ${i}-177121.bam.bcf.vcf -o vaf-mimodd/${i}-177121.bam.bcf.vcf.vaf.txt -p vaf-mimodd/${i}-177121.bam.bcf.vcf.vaf.pdf

    mkdir -p vac-mimodd
    #Step 6 - VAC
    mimodd vcf-filter ${i}-177121.bam.bcf.vcf -s ${i} 177121 --dp 10 10 -o vac-mimodd/${i}-177121.bam.bcf.dp10.vcf
    mimodd map VAC --loess-span 0 -m ${i} -c 177121 vac-mimodd/${i}-177121.bam.bcf.dp10.vcf -o vac-mimodd/${i}-177121.bam.bcf.dp10.vcf.vac.txt -p vac-mimodd/${i}-177121.bam.bcf.dp10.vcf.vac.pdf
    ######

### run the job

    sbatch --array=1-1 mimodd2.sh
    
### copy the data over the /fs3 biogrp

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/
    rsync --stats --progress --archive analysis.5/ /fs3/group/biogrp/deBono/DA0033/08_Nov_2021/analysis.5/
    
    
## Analysis.6 Call SNPs with mimodd but for 177107-177112 samples

    cd /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.6
    module load samtools/1.14

### remove marked PCR and optical duplicates for the one background strains

    samtools view -h -@12 -F 1024 ../analysis.1/bam/177122-md.bam |samtools view -@20 -Sb > 177122.no.duplicates.bam
    
### slurm array script for run mimodd in SVD mapping mode

mimodd.sh

    #!/bin/bash
    #
    #----------------------------------------------------------------
    # running a multiple independent jobs
    #----------------------------------------------------------------
    #


    #  Defining options for slurm how to run
    #----------------------------------------------------------------
    #
    #SBATCH --job-name=mimodd1_${SLURM_ARRAY_TASK_ID}
    #SBATCH --output=mimodd.sh.log
    #
    #Number of CPU cores to use within one node
    #SBATCH -c 12
    #
    #Define the number of hours the job should run. 
    #Maximum runtime is limited to 10 days, ie. 240 hours
    #SBATCH --time=48:00:00
    #
    #Define the amount of RAM used by your job in GigaBytes
    #In shared memory applications this is shared among multiple CPUs
    #SBATCH --mem=64G
    #
    #Do not requeue the job in the case it fails.
    #SBATCH --no-requeue
    #
    #Do not export the local environment to the compute nodes
    #SBATCH --export=NONE
    unset SLURM_EXPORT_ENV

    # load the respective software module(s) you intend to use
    #----------------------------------------------------------------
    module load R/4.1.2
    module load python/3.7
    module load samtools/1.14
    export PATH="/nfs/scistore16/itgrp/jelbers/.local/lib/python3.7/site-packages/MiModD/bin:$PATH"
    python3 -m MiModD.config --tmpfiles /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.5/ --snpeff /nfs/scistore16/itgrp/jelbers/bin/snpEff_v4_3t/snpEff  -t 12 -m 64 --no-use-galaxy-index-files


    # define sequence of jobs to run as you would do in a BASH script
    # use variable $SLURM_ARRAY_TASK_ID to address individual behaviour
    # in different iteration of the script execution
    #----------------------------------------------------------------
    # makes a map of the reads
    # like
    # SLURM_ARRAY_TASK_ID  sequence_name
    # 1			123456
    # 2			123457
    # 3			123458
    # and runs the array job using the sequence name

    i=`grep -P "^${SLURM_ARRAY_TASK_ID}\t" <(paste <(seq 1 6) \
    <(echo -e "177107\n177108\n177109\n177110\n177111\n177112"))|cut -f 2`

    #Step 1 - remove marked duplicates
    samtools view -h -@12 -F 1024 ../analysis.1/bam/${i}-md.bam |samtools view -@12 -Sb - > ${i}.no.duplicates.bam

    #Step 2 - merge bwa-mem2 bams
    samtools merge -f -@12 ${i}-177122.bam 177122.no.duplicates.bam ${i}.no.duplicates.bam

    #Step 3 - combined variant calling for both samples
    mimodd varcall ../analysis.1/auxData/reference-fasta.fa ${i}-177122.bam -o ${i}-177122.bam.bcf > ${i}-177122.bam.bcf.log 2>&1

    #Step 4 - extract variants
    mimodd varextract ${i}-177122.bam.bcf -o ${i}-177122.bam.bcf.vcf

    mkdir -p svd-mimodd
    #Step 5 - filter on homozygous sites
    mimodd vcf-filter ${i}-177122.bam.bcf.vcf --sample ${i} 177122 --gt 1/1 0/0 -o ${i}-177122.bam.bcf.vcf2

    #Step 6 - SVD mapping
    mimodd map SVD ${i}-177122.bam.bcf.vcf2 -o svd-mimodd/${i}-177122.bam.bcf.vcf.svd.txt -p svd-mimodd/${i}-177122.bam.bcf.vcf.svd.pdf
    ######

### run the script

    sbatch --array=1-6 mimodd.sh


### merge variants for 6 samples and background strain

    module load bcftools/1.14

    for i in `echo -e "177107\n177108\n177109\n177110\n177111\n177112"`
    do
      # use bcftools to extract just a particular sample 
      bcftools view --samples ${i} ${i}-177122.bam.bcf.vcf2 > ${i}.vcf
      # compress the extracted sample's variants
      bgzip -f -@20 ${i}.vcf
      # index the extracted sample's variants
      tabix -fp vcf ${i}.vcf.gz
      # use bcftools to get the 0/0 variants for the background strain for the current sample in the loop
      bcftools view --samples 177122 ${i}-177122.bam.bcf.vcf2 > ${i}-177122.bam.bcf.vcf2.just.177122.vcf
      # compress
      bgzip -f -@20 ${i}-177122.bam.bcf.vcf2.just.177122.vcf
      # index
      tabix -fp vcf ${i}-177122.bam.bcf.vcf2.just.177122.vcf.gz
    done

    # combine (concatenate) the background strain's 0/0 variants (some will be duplicates, hence the -Da option)
    bcftools concat -Da *-177122.bam.bcf.vcf2.just.177122.vcf.gz |bgzip -@20 -c > 177122.bam.bcf.vcf2.just.177122.vcf.gz
    tabix -p vcf 177122.bam.bcf.vcf2.just.177122.vcf.gz

    # merge all 7 files (6 mutants strains and 1 concatenated background strain)
    bcftools merge 1771{07..12}.vcf.gz 177122.bam.bcf.vcf2.just.177122.vcf.gz > 177107-177112-177122.bam.bcf.vcf2.just.177122.vcf

    module load R/4.1.2
    module load python/3.7
    module load samtools/1.14
    export PATH="/nfs/scistore16/itgrp/jelbers/.local/lib/python3.7/site-packages/MiModD/bin:$PATH"
    python3 -m MiModD.config --tmpfiles /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.5/ --snpeff /nfs/scistore16/itgrp/jelbers/bin/snpEff_v4_3t/snpEff  -t 12 -m 64 --no-use-galaxy-index-files

    # run MiModD in SVD mode
    mimodd map SVD 177107-177112-177122.bam.bcf.vcf2.just.177122.vcf -o svd-mimodd/177107-177112-177122.bam.bcf.vcf2.just.177122.vcf.svd.txt -p svd-mimodd/177107-177112-177122.bam.bcf.vcf2.just.177122.vcf.svd.pdf

### rsync over files to biogrp
    
    mkdir -p /fs3/group/biogrp/deBono/DA0033/08_Nov_2021/analysis.6/
    rsync --append-verify --stats --progress --archive /nfs/scistore16/itgrp/bioinf/projects/DA0033/08_Nov_2021/analysis.6/ /fs3/group/biogrp/deBono/DA0033/08_Nov_2021/analysis.6/ -n
